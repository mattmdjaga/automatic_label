{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from clip_retrieval.clip_client import ClipClient\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ClipClient(url=\"https://knn.laion.ai/knn-service\", indice_name=\"laion5B-H-14\")\n",
    "\n",
    "model_name = ['ViT-L/14','RN50x16']\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "models = []\n",
    "preprocesses = []\n",
    "for model_name in model_name:\n",
    "    model, preprocess = clip.load(model_name, device=device)\n",
    "    models.append(model)\n",
    "    preprocesses.append(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = ['shoe', 'handbag', 'nail polish', 'hat', 't shirt', 'coat','perfume']\n",
    "brands = ['gucci', 'prada', 'chanel', 'dior', 'versace', 'nike', 'puma',\n",
    "         'adidas', 'ralph lauren', 'armani', 'dolce & gabbana',\n",
    "         'max factor', 'loreal', '']\n",
    "colors = ['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'black',\n",
    "                'white', 'grey', 'brown', 'beige', 'gold', 'silver', 'multicolor']\n",
    "\n",
    "full_description = [f\"{color} {brand} {item}\" for item in items for brand in brands for color in colors]\n",
    "\n",
    "descriptions_tokens = []\n",
    "\n",
    "descriptions_tokens = clip.tokenize(full_description).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loop over the descriptions and embed them.\n",
    "text_embeddings = []\n",
    "n = 2048\n",
    "for i in range(len(models)):\n",
    "    with torch.no_grad():\n",
    "        temp_embeddings = []\n",
    "        for j in tqdm(range(0, len(descriptions_tokens), n)):\n",
    "            embeds = models[i].encode_text(descriptions_tokens[j:j+n])\n",
    "            temp_embeddings.append(embeds)\n",
    "    text_embeddings.append(torch.cat(temp_embeddings, dim=0))\n",
    "\n",
    "data_embeddings = torch.stack(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embeddings = data_embeddings.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del temp_embeddings, text_embeddings, descriptions_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8366"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = os.listdir('temp_imgs')\n",
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8366/8366 [10:48<00:00, 12.90it/s]\n"
     ]
    }
   ],
   "source": [
    "img_scores = []\n",
    "img_tags = []\n",
    "for i in tqdm(range(len(imgs))):\n",
    "    img = Image.open(f'temp_imgs/{imgs[i]}')\n",
    "    #img = preprocesses[0](img).unsqueeze(0).to(device)\n",
    "    top_3_scores = 0\n",
    "    tags = []\n",
    "    for model_i in range(len(models)):\n",
    "        with torch.no_grad():\n",
    "            query_tokens = preprocesses[model_i](img).unsqueeze(0).to(device)   \n",
    "            query_embedding = models[model_i].encode_image(query_tokens)\n",
    "\n",
    "            similarity = torch.nn.functional.cosine_similarity(data_embeddings[model_i], query_embedding)\n",
    "            # get the top 3 most simialr data points\n",
    "            top_10 = torch.topk(similarity, 10).indices\n",
    "            scores = torch.topk(similarity, 10).values\n",
    "            tags.append(full_description[top_10[0]])\n",
    "        top_3_scores += torch.sum(scores[:3]).item()\n",
    "    img_tags.append(tags)\n",
    "    img_scores.append(top_3_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for score in img_scores:\n",
    "    if score < 1.0:\n",
    "        i += 1\n",
    "print(round(i/len(img_scores)*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pons",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
